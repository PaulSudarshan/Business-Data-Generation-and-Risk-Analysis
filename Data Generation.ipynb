{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Gfc2_TRLrA"
   },
   "source": [
    "# Data Generation\n",
    "Assumptions that we made:\n",
    "- `TCS_Revenue` & `Client_Revenue` are generated using a mix of distrubutions like Normal, Unifrom etc.\n",
    "- Assumed some probabilty from one stage to another transit stage\n",
    "- Probability that an opportunity converts (moves from current stage to next stage) is proportional to the negative exponential of the time spent in that stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9CIzVNGcRLrE"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime as dt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "# Distributing the data points\n",
    "def weighted_pick(weights, n_picks):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return np.searchsorted(t, np.random.rand(n_picks)*s)\n",
    "\n",
    "\n",
    "# Pre Stages\n",
    "pre_stages = [('Stage 1', 0.75, 20), ('Stage 2', 0.6, 20),('Stage 3', 0.5, 20), ('Stage 4', 0.4, 20), ('Stage 5', 0.3, 20), ('Stage 6', 0.2, 20), ('Stage 7', 0.25, 20), ('Stage 8', 0.15, 20)]\n",
    "\n",
    "# Closed Stages\n",
    "closed_stages = ['Stage 9', 'Stage 10', 'Stage 11', 'Stage 12', 'Stage 13']\n",
    "\n",
    "#Success Stages\n",
    "success_stages = ['Stage 9']\n",
    "\n",
    "# Opportunity Client Attributes\n",
    "df_client = pd.DataFrame(columns=['Name','Client_Revenue', 'Region','Client_Domain','Existing_Customer'])\n",
    "\n",
    "# Total Number of Opportunities\n",
    "NUM_POINTS = 1146\n",
    "\n",
    "# Mean of Normal Distribution\n",
    "Average_Revenue = [50000,100000,150000,200000,250000]\n",
    "\n",
    "# Standard Deviation of Normal Distribution\n",
    "SD_Revenue = [5000,10000,15000, 20000, 25000]\n",
    "\n",
    "# Client Domains List (source: https://infotechlead.com/wp-content/uploads/2013/10/TCS-second-quarter-revenue.png)\n",
    "client_domains=['BFSI',\n",
    "               'Telecom',\n",
    "               'Retail & Distribution',\n",
    "               'Manufacturing',\n",
    "               'Hi-Tech',\n",
    "               'Healthcare',\n",
    "               'Travel and Hospitality',\n",
    "               'Energy & Utilities',\n",
    "               'Media',\n",
    "               'Others']\n",
    "# Region List (source: https://image.slidesharecdn.com/tcspptfinal-151119185457-lva1-app6892/95/tcs-ppt-final-12-638.jpg?cb=1447959470)\n",
    "regions = ['north america', 'middle east and africa', 'asia-pacific','india','europe','UK','latin america','others']\n",
    "\n",
    "# Reading Client Names\n",
    "clients = open('client_new.txt', 'r',encoding='utf-8').read().splitlines()\n",
    "\n",
    "# Client Revenue Generation\n",
    "sales_opps1 = [(entry.title(), np.random.choice([\n",
    "    np.random.normal(np.random.choice(Average_Revenue,p=[0.20, 0.20, 0.2, 0.2, 0.2]), \n",
    "                     np.random.choice(SD_Revenue,p=[0.20, 0.20, 0.2, 0.2, 0.2])),\n",
    "    np.random.uniform(40000,275000),\n",
    "    np.random.randint(30000,150000)],\n",
    "    p = [0.4,0.4,0.2])) for entry in np.random.choice(clients, NUM_POINTS, replace=False)]\n",
    "\n",
    "names=[]\n",
    "revenues=[]\n",
    "\n",
    "for name,revenue in sales_opps1:\n",
    "    names.append(name)\n",
    "    revenues.append(revenue)\n",
    "    \n",
    "# Distributing Client Domain to every opportunity\n",
    "domains = np.random.choice(client_domains, NUM_POINTS, replace=True,p=[0.431,0.093,0.139,0.084,0.054,0.057,0.034,0.038,0.022,0.048])\n",
    "\n",
    "# Distributing Region to every opportunity\n",
    "client_regions = np.random.choice(regions, NUM_POINTS, replace=True,p=[0.536,0.077,0.077,0.085,0.098,0.0152,0.031,0.0808000000000001])\n",
    "\n",
    "# Generating Existing customer parameter\n",
    "existing_or_no = np.random.choice([1,0], NUM_POINTS, replace=True,p=[0.7,0.3])\n",
    "\n",
    "# Assigning generated parameters to Dataframe\n",
    "df_client['Name'] = names\n",
    "df_client['Client_Revenue']=revenues\n",
    "df_client['Client_Domain']=domains\n",
    "df_client['Region']=client_regions\n",
    "df_client['Existing_Customer']=existing_or_no\n",
    "\n",
    "# Opportunity BU Attributes\n",
    "df_BU = pd.DataFrame(columns=['Name','TCS_Revenue', 'TCS_Domain'])\n",
    "\n",
    "# Mean of Normal Distribution\n",
    "Average_TCS_Revenue = [5000,10000,15000,20000,25000]\n",
    "\n",
    "# Standard Deviation of Normal Distribution\n",
    "SD_TCS_Revenue = [500,1000,1500,2000,2500]\n",
    "\n",
    "# BU's Domains List\n",
    "BUs=['Application development and maintenance',\n",
    "'Asset leverage solutions',\n",
    "'Assurance services',\n",
    "'Business process outsourcing',\n",
    "'Consulting',\n",
    "'Engineering and Industrial services',\n",
    "'Enterprise solution',\n",
    "'IT infrastructure services',\n",
    "'Cognitive Business Operations',\n",
    "'Cloud Infrastructure',\n",
    "'Automation and AI']\n",
    "\n",
    "sales_opps2 = [(entry.title(), np.random.choice([\n",
    "    np.random.normal(np.random.choice(Average_TCS_Revenue,p=[0.20, 0.20, 0.2, 0.2, 0.2]), \n",
    "                     np.random.choice(SD_TCS_Revenue,p=[0.20, 0.20, 0.2, 0.2, 0.2])),\n",
    "    np.random.uniform(4000,27500),\n",
    "    np.random.randint(3000,15000)],\n",
    "    p = [0.4,0.3,0.3])) for entry in np.random.choice(clients, NUM_POINTS, replace=False)]\n",
    "\n",
    "names=[]\n",
    "revenues=[]\n",
    "\n",
    "for name,revenue in sales_opps2:\n",
    "    names.append(name)\n",
    "    revenues.append(revenue)\n",
    "    \n",
    "# Distributing TCS Domain to every opportunity\n",
    "domains = np.random.choice(BUs, NUM_POINTS, replace=True,p=[0.4,0.05,0.05,0.06,0.04,0.05,0.05,0.1,0.1,0.05,0.05])\n",
    "\n",
    "# Assigning generated parameters to Dataframe\n",
    "df_BU['Name'] = names\n",
    "df_BU['TCS_Revenue']=revenues\n",
    "df_BU['TCS_Domain']=domains\n",
    "\n",
    "# Merging Client and BUs Dataframe into one dataframe\n",
    "new_df = pd.merge(df_client,df_BU,on = 'Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaeOc8_iRLrG"
   },
   "source": [
    "- Plotting distribution of `Client_Revenue` & `TCS_Revenue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "541a655f0d2e46b4ad62fbb2aaa230d7"
     ]
    },
    "id": "sOUwvlueRLrH",
    "outputId": "a0b71d56-926c-4236-d072-b89160ae48b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9cccfa437c4e50b7a4809502bd6ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='x', options=('Client_Revenue', 'Existing_Customer', 'TCS_Revenue')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cufflinks as cf\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.offline import init_notebook_mode\n",
    "import plotly.express as px\n",
    "\n",
    "@interact\n",
    "def scatter_plot(x=list(new_df.select_dtypes('number').columns)):\n",
    "\n",
    "    fig = px.histogram(new_df, x=x)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWqwswLVRLrI"
   },
   "source": [
    "- So, for a given stage in the data pipeline, for each opportunity left in the previous stage, for every day between when the opportunity entered the stage and now we create a multinomial line. \n",
    "\n",
    "- The multinomial line is going to return a one of the three outputs (x1,x2,x3)\n",
    "- In 99% cases it will give output as x1 that means, for every day between the opportunity entering the state and today there’s a 99% the opportunity will still be in that state at the end of the day. \n",
    "- If output is X2, means that the opportunity succeeded on that particular day (with probability given by the stage parameter)\n",
    "- If output is X3, then the opportunity died on that particular day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29KpC0N03a5z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting dataframe to a list of tuple\n",
    "sales_opportunities = [tuple(x) for x in new_df.to_records(index=False)]\n",
    "sales_opportunities\n",
    "\n",
    "# Taking a Timeline of 3 Years\n",
    "start_date = datetime.datetime.now() - datetime.timedelta(days = int(1095))\n",
    "days_range = range(365*3)\n",
    "y = [float(entry)/365. for entry in days_range]\n",
    "\n",
    "# Distributing the opportunities\n",
    "indices = weighted_pick(np.exp(y), NUM_POINTS)\n",
    "\n",
    "# Intialize every opportunity with Stage 1\n",
    "sales_data = [[pre_stages[0][0],name_value_pair[0], name_value_pair[1], name_value_pair[2],name_value_pair[3],name_value_pair[4],name_value_pair[5],name_value_pair[6], start_date + datetime.timedelta(days = int(index)),] for name_value_pair, index in zip(sales_opportunities, indices)]\n",
    "\n",
    "# Creating new dataframe remaining_opportunities_frame and sales_data_frame\n",
    "remaining_opportunities_frame = pd.DataFrame(sales_data)\n",
    "remaining_opportunities_frame.columns = ['Stage', 'Name', 'Client_Revenue', 'Region','Client_Domain','Existing_Customer', 'TCS_Revenue', 'TCS_Domain','TimeStamp']\n",
    "sales_data_frame = pd.DataFrame(sales_data)\n",
    "sales_data_frame.columns = ['Stage', 'Name', 'Client_Revenue', 'Region','Client_Domain','Existing_Customer', 'TCS_Revenue', 'TCS_Domain','TimeStamp']\n",
    "sales_data_frame.head()\n",
    "\n",
    "# Adding trasition stages in sales_data_frame\n",
    "finished_list = set([])\n",
    "idx = 0\n",
    "for stage_index, stage in enumerate(pre_stages[1:]):\n",
    "    idx = idx + 1\n",
    "    next_stage = pd.DataFrame([(sales_opp[1], index, np.argmax(entry)) \n",
    "                               for sales_opp in sales_data \n",
    "                               for index, entry in enumerate(\n",
    "                                   np.random.multinomial(1, [0.99, (1. - stage[1])/100., \n",
    "                                                             stage[1]/100.0], \n",
    "                                                         (datetime.datetime.now() - sales_opp[8]).days)) \n",
    "                               if entry[0] != 1 and sales_opp[1] not in finished_list])\n",
    "\n",
    "    next_stage.columns = ['Name', 'TimeStamp', 'Status']\n",
    "    meh = next_stage.iloc[next_stage.groupby('Name').TimeStamp.idxmin()]\n",
    "    tempy_frame = meh.merge(remaining_opportunities_frame[['Name','Client_Revenue', 'Region','Client_Domain','Existing_Customer', 'TCS_Revenue', 'TCS_Domain','TimeStamp',]], how='inner', on='Name')\n",
    "    tempy_frame['new_date'] = tempy_frame.apply(lambda x: sales_data_frame[sales_data_frame['Name']==x.Name]['TimeStamp'].tolist()[-1] + datetime.timedelta(days = x.TimeStamp_x), axis=1)\n",
    "    tempy_frame = tempy_frame[['Name', 'Status', 'Client_Revenue', 'Region','Client_Domain',\n",
    "       'Existing_Customer','TCS_Revenue', 'TCS_Domain','new_date']]\n",
    "    tempy_frame.columns = ['Name', 'Status', 'Client_Revenue', 'Region','Client_Domain',\n",
    "       'Existing_Customer','TCS_Revenue', 'TCS_Domain', 'TimeStamp']\n",
    "\n",
    "    success_frame = tempy_frame[tempy_frame.Status == 1]\n",
    "    success_frame = success_frame.drop('Status', 1)\n",
    "    success_frame.insert(0, 'Stage', pre_stages[stage_index + 1][0] if stage_index + 1 < len(pre_stages) else success_stages[0])\n",
    "\n",
    "    failure_frame = tempy_frame[tempy_frame.Status == 2]\n",
    "    failure_frame = failure_frame.drop('Status', 1)\n",
    "    failure_frame.insert(0, 'Stage', np.random.choice(closed_stages, p=[0.3,0.2,0.2,0.2,0.1]))\n",
    "    sales_data_frame = pd.concat([sales_data_frame,failure_frame,success_frame])\n",
    "\n",
    "    finished_frame = sales_data_frame.groupby('Name').apply(lambda x: x.Stage.isin(closed_stages).any())\n",
    "    finished_list = set(finished_list).union(set(finished_frame[finished_frame == True].index.values))\n",
    "    remaining_opportunities = remaining_opportunities_frame[~remaining_opportunities_frame.Name.isin(finished_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "9sOOTNLSRLrJ"
   },
   "outputs": [],
   "source": [
    "# Converting into DateTime format and sorting dataframe by Name & TimeStamp \n",
    "sales_data_frame['TimeStamp'] = pd.to_datetime(sales_data_frame['TimeStamp'],format='%d-%m-%Y %H:%M')\n",
    "sales_data_frame= sales_data_frame.sort_values(by = ['Name','TimeStamp'])\n",
    "\n",
    "# Assigning ID to every opportunity\n",
    "uniq = sales_data_frame['Name'].unique()\n",
    "uniq_id=[]\n",
    "id_=0\n",
    "for i in uniq:\n",
    "    for j in sales_data_frame['Name']:\n",
    "        if i==j:\n",
    "            uniq_id.append(id_)\n",
    "    id_+=1\n",
    "sales_data_frame['ID'] = uniq_id\n",
    "\n",
    "# Downloading the dataset\n",
    "# sales_data_frame.to_csv('experimental_data.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df = sales_data_frame.copy()\n",
    "my_df  = pd.DataFrame()\n",
    "arr = df.Name.unique()\n",
    "for x in arr:\n",
    "    new_df = df[df.Name == x]\n",
    "    new_df = new_df.reset_index(drop = 1)\n",
    "    k = list(new_df['Stage']) \n",
    "    new_df[\"NewTimeStamp\"] = \"\"\n",
    "    z = 0\n",
    "    for i in range(0,len(k)):\n",
    "        end_date = new_df['TimeStamp'][0] + datetime.timedelta(days=z)\n",
    "        new_df.loc[i,'NewTimeStamp']= end_date\n",
    "        z = z + 20\n",
    "    my_df = my_df.append(new_df)\n",
    "\n",
    "my_df['TimeStamp'] = my_df['NewTimeStamp']\n",
    "my_df.drop(['NewTimeStamp'],axis = 1, inplace = True)\n",
    "# Saving the dataset\n",
    "my_df.to_csv('exp_2.csv', sep=',', index=False,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5wlUpKeoGcc",
    "scrolled": true
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data_Model-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
